# -*- coding: utf-8 -*-
# """Cuey

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1wTBksjYnd4kZE5iAVXCTRi-iO3UIq7cV
# """

#STEP 1 : Install the dependencies (already done)

# STEP 2: Import Libraries
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch
import json
import os
import random

# Disable W&B logging
os.environ["WANDB_DISABLED"] = "true"

# STEP 3: Load dataset from JSONL
with open("dataset.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

dataset = Dataset.from_list(lines)

# STEP 4: Format dataset for instruction-tuning
def format_prompt(example):
    return {
        "text": (
            f"You are Cuey, a professional pool and snooker coach.\n"
            f"Question: {example['prompt']}\n"
            f"Answer: {example['response']}"
        )
    }

formatted_dataset = dataset.map(format_prompt)

# STEP 5: Tokenize formatted data
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    tokens = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=256,
    )
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_dataset = formatted_dataset.map(tokenize)

# STEP 6: Load model in 4-bit for memory efficiency
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# Enable gradient checkpointing to save more memory
base_model.gradient_checkpointing_enable()

# Prepare model for k-bit LoRA training
base_model = prepare_model_for_kbit_training(base_model)

# STEP 7: Apply LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)

# STEP 8: Define training arguments
training_args = TrainingArguments(
    output_dir="./cuey_model",
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_steps=1,
    save_total_limit=1,
    fp16=True,
    learning_rate=2e-4,
    report_to="none"
)

# STEP 9: Train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

trainer.train()

# STEP 10: Define chat function for Cuey
def chat_with_cuey(prompt):
    system_prompt = (
    "You are Cuey, a world-class pool and snooker coach. You give clear, concise, and technically accurate answers tailored to players of all skill levels.\n"
    "Do not make up rules or techniques. If unsure, say 'I'm not sure' rather than guessing.\n"
    "Use step-by-step advice when helpful, and always stay on-topic.\n"
    "Answer ONLY the question provided. Do NOT ask follow-up questions.\n"
    "Question: "
    )

    full_prompt = system_prompt + prompt + "\nAnswer:"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.75,
        top_p=0.9,
        repetition_penalty=1.2
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip()

# STEP 11: Test Cuey after training
for i in range(0,5): #sampling 5 random prompts
  sample_prompt = dataset[random.randint(1,100)]["prompt"]
  reply = chat_with_cuey(sample_prompt)
  print("")
  print(f"Cuey says: {reply}")

with open("dataset.jsonl", "r") as f:
    for i, line in enumerate(f):
        print(f"Line {i+1}: {line}")
        if i > 10:
            break
